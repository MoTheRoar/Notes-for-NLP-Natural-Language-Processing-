{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc1 = nlp(u'Apple to build a Hong Kong factory for $6 million')\n",
    "\n",
    "for token in doc1:\n",
    "    print(token.text, end= ' | ')\n",
    "    \n",
    "for entity in doc1.ents:\n",
    "    print(entity)\n",
    "    print(entity.label_)\n",
    "    print(str(spacy.explain(entity.label_)))\n",
    "    print('\\n')\n",
    "    \n",
    "text = \"\"\"This is Mo's text, isn't it?\"\"\"\n",
    "\n",
    "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "tokenizer.tokenize(text)\n",
    "\n",
    "# ['This', 'is', 'Mo's', 'text,', 'isn't', 'it?']\n",
    "\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokenizer.tokenize(text)\n",
    "\n",
    "# ['This', 'is', 'Mo', 's', 'text', ',', 'is', 'n't', 'it', '?']\n",
    "\n",
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "tokenizer.tokenize(text)\n",
    "\n",
    "# ['This', 'is', 'Mo', \"'\", 's', 'text', ',', 'isn', \"'\", 't', 'it', '?']\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.stemPorterStemmer\n",
    "    #feet -> feet\n",
    "    #cats -> cat\n",
    "    #wolves -> wolv\n",
    "    #--Fails on irregular forms, produces non-words\n",
    "\n",
    "# WordNet lemmatizer: uses WordNet Database to lookup lemmas(base words)\n",
    "# nltk.stem.WordNetLemmatizer\n",
    "    #feet -> feet\n",
    "    #cats -> cat\n",
    "    #wolves -> wolf\n",
    "    #talked -> talked\n",
    "    #--Not all forms are reduced\n",
    "    \n",
    "import nltk\n",
    "text = 'Feet cats wolves talked'\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "' '.join(stemmer.stem(token) for token in tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I) Feature Extraction from Text\n",
    "\n",
    "-High frequency n-grams:\n",
    "--Articles, prepositions, etc(and, a, the)\n",
    "--They are called stop words because they have no significance in the context of the sentence\n",
    "\n",
    "-Low frequency n-grams:\n",
    "--Typos, rare n-grams.\n",
    "--We don't need them, otherwise they will likely overfit.\n",
    "\n",
    "-Medium frequency n-grams:\n",
    "--Those good n-gram\n",
    "\n",
    "Let's remove some n-grams based on their occurrence frequency in our document corpus (how many documents have a particular n-gram divided by the total number of documents). Which can be removed?\n",
    "-High and Low Frequency -> words that are meaningless and types/rarely used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embedding\n",
    "\"\"\"\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "from processing import most_common_words, vector_list\n",
    "\n",
    "# print word and vector representation at index 347\n",
    "print(most_common_words[500])\n",
    "\n",
    "# define find_closest_words\n",
    "def find_closest_words(word_list, vector_list, word_to_check):\n",
    "    return sorted(word_list,\n",
    "                  key=lambda x: cosine(vector_list[word_list.index(word_to_check)], vector_list[word_list.index(x)]))[:10]\n",
    "\n",
    "# find closest words to food\n",
    "close_to_food = find_closest_words(most_common_words, vector_list, 'food')\n",
    "print(close_to_food)\n",
    "    \n",
    "\"\"\"\n",
    "    \n",
    "    \n",
    "myfile = open(r'Coordinates.txt')\n",
    "myfile.read()\n",
    "myfile.seek(0)\n",
    "myfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "mystring = \"\"\"Amazon just bought Ubisoft for $6 billion\"\"\"\n",
    "doc1 = nlp(mystring)\n",
    "\n",
    "for ent in doc1.ents:\n",
    "    expLabel = str(spacy.explain(ent.label_))\n",
    "    # print(ent.text + \" - \" + ent.label_ + \" - \" + str(spacy.explain(ent.label_)))\n",
    "    print(f\"{ent.text:20} {ent.label_:20} {expLabel:20}\")\n",
    "    # print(f'{ent.text:10} {ent.label_:8} {expLabel:7}')\n",
    "    \n",
    "for ent in doc1.ents:\n",
    "    explainEnt = str(spacy.explain(ent.label_))\n",
    "    print(f\"{ent.text:10} {ent.label_:10} {explainEnt:10}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "p_stemmer = PorterStemmer()\n",
    "sents = \"Running watering fainting faint fate mate mating generation generate\"\n",
    "tokenize = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokens = tokenize.tokenize(sents)\n",
    "for sent in tokens:\n",
    "    print(f'{sent:20} {p_stemmer.stem(sent):20}')\n",
    "\n",
    "\n",
    "\n",
    "words = ['Running', \"Watering\", \"Fainting\", \"faint\", \"fate\", \"mate\", \"mating\", \"generation\", \"generate\"]\n",
    "print('\\n')\n",
    "for word in words:\n",
    "    print(f\"{word:20} {p_stemmer.stem(word):20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Years Old\n",
    "# Years-old\n",
    "\n",
    "pattern1 = {'LOWER': 'years'}, {'LOWER': 'old'}\n",
    "pattern2 = {'LOWER': 'years'}, {'IS_PUNCT':True}, {'LOWER':'old'}\n",
    "\n",
    "matcher.add('Years Old', None, pattern1, pattern2)\n",
    "doc = nlp(u'Today I just turned 4 Years Old. Next year I will be five-years-old')\n",
    "found_matches = matcher(doc)\n",
    "print(found_matches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc1 = nlp(u'Wow that was crazy. Apple I think I did pretty well on my physics exam! That cost me $6 Billion')\n",
    "    \n",
    "# Count the frequencies of different coarse-grained POS tags:\n",
    "POS_counts = doc1.count_by(spacy.attrs.POS)\n",
    "\n",
    "for ent in doc1.ents:\n",
    "    explainEnt = str(spacy.explain(ent.label_))\n",
    "    print(f\"{ent.text:20} {ent.label_:10} {explainEnt:10}\")\n",
    "\n",
    "print(POS_counts)\n",
    "\n",
    "\n",
    "\n",
    "for k, v in sorted(POS_counts.items()):\n",
    "    print(f\"{k} {doc1.vocab[k].text:10} {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WITHOUT A PRETRAINED MODEl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "\n",
    "# WITHOUT A PRETRAINED MODEL\n",
    "\n",
    "df = pd.read_csv('redditWorldNews.csv')\n",
    "df.head(10)\n",
    "\n",
    "newsTitles = df['title'].values\n",
    "newsTitles\n",
    "\n",
    "# nltk.download('punkt')\n",
    "newsVec = [nltk.word_tokenize(title) for title in newsTitles]\n",
    "newsVec\n",
    "\n",
    "\"\"\"model = Word2Vec(newsVec, min_count=1, size=32)\n",
    "model.most_similar('men')\n",
    "\n",
    "vec = model['king'] - model['man'] + model['woman']\n",
    "model.most_similar([vec])   \"\"\"\n",
    "\n",
    "model = Word2Vec(newsVec, min_count=1, vector_size=200)\n",
    "\n",
    "word = 'polite'\n",
    "print(model.wv.most_similar(positive=word, topn=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WITH A PRETRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True, limit=10000)\n",
    "\n",
    "vec = model['king'] - model['man'] + model['woman']\n",
    "vec\n",
    "# model.most_similar([vec])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "posts = []\n",
    "reddit = praw.Reddit(client_id='QQkRGZfy1EZLGUDza4NJvw', client_secret='IGa8eb7nt9SVW_PHP_1kX_m_kClvKA', user_agent='Webscrape')\n",
    "\n",
    "# Get first 20 hot posts from r/LeagueOfLegends\n",
    "hot_posts = reddit.subreddit('leagueoflegends').hot(limit=100)\n",
    "for post in hot_posts:\n",
    "    posts.append([post.title, post.score, post.subreddit, post.id, post.url, post.num_comments, post.selftext])\n",
    "\n",
    "posts = pd.DataFrame(posts, columns=['title', 'score', 'subreddit',  'id', 'url', 'num_comments', 'body'])\n",
    "\n",
    "worldTitles = posts['title'].values\n",
    "worldVec = [nltk.word_tokenize(title) for title in worldTitles]\n",
    "\n",
    "# similar_words = Word2Vec(worldVec, min_count=1, vector_size=200)\n",
    "sim_words = []\n",
    "# for i in worldVec[0]:\n",
    "    # word = i\n",
    "    # sim_words.append(similar_words.wv.most_similar(positive=word, topn=1))\n",
    "\n",
    "print(posts)\n",
    "# print(sim_words)\n",
    "\n",
    "# print(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def show_ents(doc):\n",
    "    if doc.ents:\n",
    "        for ent in doc.ents:\n",
    "            explanation = str(spacy.explain(ent.label_))\n",
    "            print(f'{ent.text:10} {ent.label_:10} {explanation:10}')\n",
    "    else:\n",
    "        print('Not Found')\n",
    "        \n",
    "doc = nlp(u'Tesla was worth $400 in the 1900s. Crazy that Lincoln, the previous president, banned it.')\n",
    "show_ents(doc)\n",
    "\n",
    "\n",
    "# Get the hash for ORG\n",
    "ORG = doc.vocab.strings[u'ORG']\n",
    "\n",
    "# Create a Span for the new entity\n",
    "new_ent = Span(doc, 0, 1, label=ORG)\n",
    "\n",
    "# Add the entity to the existing DOC object \n",
    "doc.ents = list(doc.ents) + [new_ent]\n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "# show_ents(doc)\n",
    "\n",
    "\n",
    "doc = nlp(u'Our company created a brand new vacuum cleaner.' u\"This new vacuum-cleaner is the best in show.\")\n",
    "show_ents(doc)\n",
    "\n",
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "phrase_list = ['vacuum cleaner', 'vacuum-cleaner']\n",
    "phrase_patterns = [nlp(text) for text in phrase_list]\n",
    "matcher.add('newproduct', None,*phrase_patterns)\n",
    "\n",
    "found_matches = matcher(doc)\n",
    "found_matches\n",
    "\n",
    "\n",
    "# from spacy.tokens import Span\n",
    "PROD = doc.vocab.strings[u'PRODUCT']\n",
    "new_ents = [Span(doc, match[1], match[2], label=PROD) for match in found_matches]\n",
    "# print(new_ents)\n",
    "print(doc.ents)\n",
    "doc.ents = list(doc.ents) + new_ents\n",
    "print(doc.ents)\n",
    "\n",
    "displacy.serve(doc, style='ent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(u'This is my first sentence. This is my second sentence. This is my third sentence.')\n",
    "# doc_sents = [sent for sent in doc.sents]\n",
    "\n",
    "doc1 = nlp(u\"'Some say that life gives lemons; I say it does not.' - Mohamed Ilaiwi\")\n",
    "\n",
    "for sent in doc1.sents:\n",
    "    print(sent)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@Language.component('component')\n",
    "def set_custom_component(doc):\n",
    "    for token in doc[:-1]:\n",
    "        if token.text == \";\":\n",
    "            doc[token.i+1].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe('component', before='parser')\n",
    "nlp.pipe_names\n",
    "    \n",
    "\n",
    "doc1 = nlp(u\"'Some say that life gives lemons; I say it does not.' - Mohamed Ilaiwi\")\n",
    "\n",
    "for sent in doc1.sents:\n",
    "    print(sent)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = [0,0,1,1,1,2,2,3,3,4]\n",
    "num1 = []\n",
    "for i in range(len(nums)):\n",
    "    try:\n",
    "        if nums[i] == nums[i+1] or nums[i] == nums[i-1]:\n",
    "        # temp = nums[counter]\n",
    "            nums.append(nums.pop(i))\n",
    "    except:\n",
    "        print(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import os\n",
    "\n",
    "# print(os.path.isfile(r'TextFiles\\smsspamcollection.tsv'))\n",
    "\n",
    "df = pd.read_csv(r'TextFiles\\smsspamcollection.tsv', sep='\\t')\n",
    "df.head()\n",
    "\n",
    "# df.isnull().sum()\n",
    "# df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search(arr, low, high, x):\n",
    " \n",
    "    # Check base case\n",
    "    if high >= low:\n",
    " \n",
    "        mid = (high + low) // 2\n",
    " \n",
    "        # If element is present at the middle itself\n",
    "        if arr[mid] == x:\n",
    "            return mid\n",
    " \n",
    "        # If element is smaller than mid, then it can only\n",
    "        # be present in left subarray\n",
    "        elif arr[mid] > x:\n",
    "            return binary_search(arr, low, mid - 1, x)\n",
    " \n",
    "        # Else the element can only be present in right subarray\n",
    "        else:\n",
    "            return binary_search(arr, mid + 1, high, x)\n",
    " \n",
    "    else:\n",
    "        # Element is not present in the array\n",
    "        return -1\n",
    "\n",
    "nums = [-1, 0, 3, 5, 9 ,12]\n",
    "binary_search(nums, 0, len(nums)-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = [[1,3],[2,6],[8,10],[15,18]]\n",
    "\n",
    "# lst = list(intervals[0]) + list(intervals[1])\n",
    "\"\"\"for i in range(len(intervals)-1):\n",
    "    if (intervals[i][1] - intervals[i+1][0]) >= 0:\n",
    "        lst.append(intervals[i][1])\"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform imports and load the dataset:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'TextFiles\\smsspamcollection.tsv', sep='\\t')\n",
    "df.head()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['message']  # this time we want to look at the text\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# COUNT VECTORIZER\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape\n",
    "\n",
    "# print(X_train_counts.shape)\n",
    "\n",
    "# TFIDF TRANSFORMER\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape\n",
    "\n",
    "# print(X_train_tfidf.shape)\n",
    "\n",
    "# TFIDF VECTORIZER\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# pred = clf.predict(X_test)\n",
    "# pred.predict(['Baby come home'])\n",
    "\n",
    "# Instead of having to fit_transform and count vectorizer on your test data to call the predict, you can just call a pipeline step\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()), ('clf', LinearSVC())])\n",
    "text_clf.fit(X_train, y_train)\n",
    "predictions = text_clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "# print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "# print(classification_report(y_test, predictions))\n",
    "\n",
    "text_clf.predict([\"CALL ME BABY\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "text1 = \"Tracy loves writing about data science\"\n",
    "text2 = \"Tracy loves posing videos about this\"\n",
    "text3 = \"I can't wait to go to the moon!\"\n",
    "text4 = \"You might have missed the moon, but we're still headed for mars.\"\n",
    "\n",
    "corpus = [text1, text2, text3, text4]\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "word_count_vec = vect.fit_transform(corpus)\n",
    "print(word_count_vec.shape)\n",
    "print(vect.get_feature_names_out())\n",
    "print(\"Vocabulary: \", vect.vocabulary_)\n",
    "print(word_count_vec.toarray())\n",
    "\n",
    "\n",
    "# vector = vect.transform(corpus)\n",
    "# print(vector)\n",
    "\n",
    "# Use the conent column instead of a single text variable\n",
    "# matrix = vect.fit_transform(corpus) Which is basically just word_count_vec\n",
    "counts = pd.DataFrame(word_count_vec.toarray(), index=['doc1', 'doc2', 'doc3', 'doc4'], columns=vect.get_feature_names_out())\n",
    " \n",
    " # Using TFIDF Transformer to focus on more relevant data\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "tf_transformer = TfidfTransformer().fit(word_count_vec)\n",
    "word_count_vec_tf = tf_transformer.transform(word_count_vec)\n",
    "\n",
    "df1 = pd.DataFrame(word_count_vec_tf.toarray(), index=['doc1', 'doc2', 'doc3', 'doc4'], columns=vect.get_feature_names_out())\n",
    "df1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "doc1 = \"'That is to say,' replied Martin, 'that there is some pleasure having no pleasure'\"\n",
    "doc2 = \"'It is always well to hope,' said Martin\"\n",
    "doc3 = \"'Whereof one cannot speak, thereof one must be silent' - Ludwig Wittgenstein\"\n",
    "doc4 = \"'The unexamined life is not worth living' - Socrates.\"\n",
    "\n",
    "corpus = [doc1, doc2, doc3, doc4]\n",
    "\n",
    "vect = CountVectorizer()\n",
    "word_count_vect = vect.fit_transform(corpus)\n",
    "# print(vect.get_feature_names_out())\n",
    "# print('Vocabulary: ', vect.vocabulary_)\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(word_count_vect.toarray(), index=['doc1', 'doc2', 'doc3', 'doc4'], columns=vect.get_feature_names_out())\n",
    "df\n",
    "\n",
    "tfidf_transformers = TfidfTransformer()\n",
    "tfidf = tfidf_transformers.fit_transform(word_count_vect)\n",
    "\n",
    "\n",
    "df1 = pd.DataFrame(tfidf.toarray(), index=['doc1', 'doc2', 'doc3', 'doc4'], columns=vect.get_feature_names_out())\n",
    "df1\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.string_)\n",
    "\n",
    "\n",
    "df = pd.read_csv(r'TextFiles\\complaints.csv')\n",
    "# df.head()\n",
    "# n1 = math.nan\n",
    "df = clean_dataset(df)\n",
    "\n",
    "# What is our training data -> X = product, y = complaint\n",
    "X = df['Product']\n",
    "y = df['Consumer complaint narrative'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.33)\n",
    "\n",
    "vect = CountVectorizer()\n",
    "word_count_vec = vect.fit_transform(X_train)\n",
    "print(word_count_vec.toarray()[:5])\n",
    "\n",
    "tfidf = TfidfTransformer().fit(word_count_vec)\n",
    "tfidf_now_vec = tfidf.transform(word_count_vec)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "linear_svc = LinearSVC()\n",
    "clf = linear_svc.fit(tfidf_now_vec, y_train)\n",
    "\n",
    "\n",
    "\n",
    "to_predict = [\"I have outdated information on my credit report that I have previously disputed thas has yet to be removed\"]\n",
    "\n",
    "\n",
    "# y_pred = clf.predict(X_test)\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is a movie review positive or negative project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(r'TextFiles\\moviereviews.tsv', sep='\\t')\n",
    "df.head()\n",
    "len(df)\n",
    "\n",
    "# df['review'][2] --> Positive\n",
    "df.isnull().sum()\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "# Remove any spaces\n",
    "blanks = []\n",
    "\n",
    "# (index, label, review text)\n",
    "for i,lb,rv, in df.itertuples():\n",
    "    if rv.isspace():\n",
    "        blanks.append(i)\n",
    "# Blanks contains all the indexes of empty strings in df        \n",
    "df.drop(blanks,inplace=True)\n",
    "len(df)\n",
    "\n",
    "# Split into training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['review']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42, test_size=.33)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()), ('clf', LinearSVC())])\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "predictions = text_clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Class Reports\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(accuracy_score(y_test, predictions))\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantics and Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(r'en_core_web_lg\\en_core_web_lg-3.2.0')\n",
    "\n",
    "nlp(u'The quick brown fox jumped').vector.shape\n",
    "# Returns 300. 300 means that there are 300 dimensions in the vector in the document. The document is just the average of all the singular vectors\n",
    "\n",
    "nlp(u'fox').vector.shape\n",
    "tokens = nlp(u'like love hate')\n",
    "print(f'{\"Tok1\":10} {\"Tok2\":10} {\"Similarity\":10}')\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        # print(token1.text, token2.text, token1.similarity(token2))      \n",
    "        print(f'{token1.text:10} {token2.text:10} {token1.similarity(token2):10}')\n",
    "\n",
    "nlp.vocab.vectors.shape\n",
    "# (684831, 300) --> (amtOfWords, dimensions)\n",
    "\n",
    "tokens = nlp(u'dog cat John')\n",
    "print('\\n')\n",
    "# OOV = Out of Vocabulary\n",
    "for token in tokens:\n",
    "    print(f\"{token.text:10} {token.has_vector:10} {token.vector_norm:10} {token.is_oov:10}\")\n",
    "    \n",
    "    \n",
    "from scipy import spatial \n",
    "cosine_similarity = lambda vec1, vec2: 1-spatial.distance.cosine(vec1, vec2)\n",
    "\n",
    "king = nlp.vocab['king'].vector\n",
    "man = nlp.vocab['man'].vector\n",
    "woman = nlp.vocab['woman'].vector\n",
    "\n",
    "new_vector = king - man + woman\n",
    "computed_similarities = []\n",
    "\n",
    "# FOR ALL WORDS IN MY VOCAB. Basically all 684000 words\n",
    "\"\"\"\n",
    "for loop through nlp.vocab.vectors gives back the hash value of the lexemes\n",
    "we need to first transform the hash value back to lexeme object\n",
    "\"\"\"\n",
    "for ID in nlp.vocab.vectors:\n",
    "\n",
    "    word = nlp.vocab[ID]\n",
    "    if word.has_vector:\n",
    "        if word.is_lower:\n",
    "            # Is the word a number\n",
    "            if word.is_alpha:\n",
    "                similarity = cosine_similarity(new_vector, word.vector)\n",
    "                computed_similarities.append((word, similarity))\n",
    " \n",
    " \n",
    "# -item[1] is descending order. Descending gives you most similar, ascending gives you least similar\n",
    "computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n",
    "sim = [t[0].text for t in computed_similarities[:10]]\n",
    "sim\n",
    "new_vector\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "nlp = spacy.load(r'site-packages\\en_core_web_lg\\en_core_web_lg-3.2.0')\n",
    "\n",
    "def most_similar(vect):\n",
    "    vector = nlp.vocab[vect].vector\n",
    "    vect1 = nlp(vect)\n",
    "    \n",
    "    word_similarity = lambda vec1, vec2: 1- spatial.distance.cosine(vec1, vec2)\n",
    "    common_similarities = []\n",
    "    \n",
    "    for ID in nlp.vocab.vectors:\n",
    "        word = nlp.vocab[ID]\n",
    "        if word.has_vector:\n",
    "            if word.is_lower:\n",
    "                if word.is_alpha:\n",
    "                    similarity = word_similarity(vector, word.vector)\n",
    "                    common_similarities.append((word, similarity))\n",
    "                    \n",
    "    \n",
    "    sort_common_sim = sorted(common_similarities, key= lambda item: -item[1])\n",
    "    sim = [t[0].text for t in sort_common_sim[:10]]\n",
    "    get_sim = [vect1.similarity(nlp(i)) for i in sim]\n",
    "    return sim, get_sim\n",
    "\n",
    "\n",
    "most_similar('compuetwe')\n",
    "    \n",
    "                    \n",
    "                    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(r'TextFiles\\complaints.csv')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df.isnull().sum()\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "y = df['Product']\n",
    "X = df['Consumer complaint narrative']\n",
    "\n",
    "\n",
    "for item, frame in df['Product'].iteritems():\n",
    "    if not pd.notnull(frame):\n",
    "        print(item, frame)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.33)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()), ('clf', LinearSVC())])\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "predictions = text_clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# print(confusion_matrix(y_test, predictions))\n",
    "# print(classification_report(y_test, predictions))\n",
    "# print(accuracy_score(y_test, predictions))\n",
    "       \n",
    "print(text_clf.predict(['I have outdated information on my credit report that I have previously disputed thas has yet to be removed']))\n",
    "df['Product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "a = 'This is a good movie'\n",
    "sid.polarity_scores(a)\n",
    "\n",
    "a = 'This was the best, most awesome movie EVER MADE!!!!'\n",
    "sid.polarity_scores(a)\n",
    "\n",
    "a = \"This was the WORST movie I has ever disgraced the screen.\"\n",
    "sid.polarity_scores(a)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(r'TextFiles\\amazonreviews.tsv', sep='\\t')\n",
    "df.head()\n",
    "\n",
    "# To see how many positive or negative reviews we have\n",
    "df['label'].value_counts()\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "blanks = []\n",
    "for i, lb, rv in df.itertuples():\n",
    "    # (index, label, review)\n",
    "    if type(rv) == str:\n",
    "        if rv.isspace():\n",
    "            blanks.append(i)\n",
    "\n",
    "# NO blanks but if we did it would be: \n",
    "# df.drop(blanks, inplace=True)\n",
    "sid.polarity_scores(df.iloc[0]['review'])\n",
    "\n",
    "df['scores'] = df['review'].apply(lambda review: sid.polarity_scores(review))\n",
    "df.head()\n",
    "\n",
    "df['compound'] = df['scores'].apply(lambda d: d['compound'])\n",
    "df.head()\n",
    "\n",
    "df['comp_score'] = df['compound'].apply(lambda score: 'pos' if score >= 0 else 'neg')\n",
    "df.head()\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "print(accuracy_score(df['label'], df['comp_score']))\n",
    "print(classification_report(df['label'], df['comp_score']))\n",
    "print(confusion_matrix(df['label'], df['comp_score']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'TextFiles\\moviereviews.tsv', sep='\\t')\n",
    "df.head()\n",
    "df['label'].value_counts()\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "blanks = []\n",
    "for i, lb, rv in df.itertuples():\n",
    "    if type(rv) == str:\n",
    "        if rv.isspace():\n",
    "            blanks.append(i)\n",
    "            \n",
    "df.drop(blanks, inplace=True)\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "df['scores'] = df['review'].apply(lambda review: sid.polarity_scores(review))\n",
    "df.head()\n",
    "\n",
    "df['compound'] = df['scores'].apply(lambda d: d['compound'])\n",
    "df.head()\n",
    "\n",
    "df['comp_score'] = df['compound'].apply(lambda score: 'pos' if score >= 0 else 'neg')\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "print(accuracy_score(df['label'], df['comp_score']))\n",
    "print(classification_report(df['label'], df['comp_score']))\n",
    "print(confusion_matrix(df['label'], df['comp_score']))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "import random\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "posts = []\n",
    "reddit = praw.Reddit(client_id='', client_secret='', user_agent='Webscrape')\n",
    "\n",
    "\n",
    "\n",
    "# Get first 20 hot posts from r/LeagueOfLegends\n",
    "hot_posts = reddit.subreddit('all').hot(limit=100)\n",
    "for post in hot_posts:\n",
    "    # posts.append([post.title, post.score, post.subreddit, post.id, post.url, post.num_comments, post.selftext])\n",
    "    posts.append([post.title, post.subreddit])\n",
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "import random\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "posts = []\n",
    "reddit = praw.Reddit(client_id='QQkRGZfy1EZLGUDza4NJvw', client_secret='IGa8eb7nt9SVW_PHP_1kX_m_kClvKA', user_agent='Webscrape')\n",
    "\n",
    "\n",
    "\n",
    "# Get first 20 hot posts from r/LeagueOfLegends\n",
    "hot_posts = reddit.subreddit('all').hot(limit=100)\n",
    "for post in hot_posts:\n",
    "    # posts.append([post.title, post.score, post.subreddit, post.id, post.url, post.num_comments, post.selftext])\n",
    "    posts.append([post.title, post.subreddit])\n",
    "        \n",
    "# posts = pd.DataFrame(posts, columns=['title', 'score', 'subreddit',  'id', 'url', 'num_comments', 'body'])\n",
    "posts = pd.DataFrame(posts, columns=['title', 'subreddit'])\n",
    "\n",
    "worldTitles = posts['title'].values\n",
    "def clean_up(titles=posts['title'].values):\n",
    "    non_punct = []\n",
    "    stop_words = set(nlp.Defaults.stop_words)\n",
    "    \n",
    "    for i in worldTitles:\n",
    "        tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "        new_words = tokenizer.tokenize(i)\n",
    "        filtered = [w for w in new_words if not w.lower() in stop_words]\n",
    "        # print(new_words)\n",
    "        non_punct.append(filtered)\n",
    "    return non_punct\n",
    "\n",
    "\n",
    "cv = CountVectorizer(max_df = .9, min_df=2, stop_words='english')\n",
    "dtm = cv.fit_transform(posts['title'])\n",
    "\n",
    "# LDA approach    \n",
    "LDA = LatentDirichletAllocation(n_components=7, random_state=42)\n",
    "LDA.fit(dtm)    \n",
    "\n",
    "# NMF approach\n",
    "tfidf = TfidfVectorizer(max_df=9, min_df=2, stop_words='english')\n",
    "dtm1 = tfidf.fit_transform(posts['title'])\n",
    "\n",
    "nmf_model = NMF(n_components=7, random_state=42)\n",
    "nmf_model.fit(dtm1)\n",
    "\n",
    "\n",
    "#import random\n",
    "random_word_id = random.randint(0, len(cv.get_feature_names_out()))\n",
    "\n",
    "\n",
    "# Get the topics\n",
    "single_topic = LDA.components_[0]\n",
    "\n",
    "top_five_words = single_topic.argsort()[-10:]\n",
    "\n",
    "\n",
    "for index , topic in enumerate(LDA.components_):\n",
    "    print(f\"THE TOP 10 WORDS FOR TOPIC #{index} BY LDA\")\n",
    "    print([cv.get_feature_names_out()[index] for index in topic.argsort()[-15:]])\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "for index, topic in enumerate(nmf_model.components_):\n",
    "    print(f\"THE TOP 15 WORDS FOR TOPIC #{index} BY NMF\")\n",
    "    print([tfidf.get_feature_names_out()[index] for index in topic.argsort()[-15:]])\n",
    "    print('\\n')\n",
    "   \n",
    "topic_result = nmf_model.transform(dtm1)\n",
    "\n",
    "#----------------------------------- FOR DATAFRAME ------------------------------------#\n",
    "posts['scores'] = posts['title'].apply(lambda title: sid.polarity_scores(title))\n",
    "\n",
    "# posts['negative'] = posts['scores'].apply(lambda p: p['neg'])\n",
    "# posts['neutral'] = posts['scores'].apply(lambda p: p['neu'])\n",
    "# posts['positive'] = posts['scores'].apply(lambda p: p['pos'])\n",
    "posts['compound'] = posts['scores'].apply(lambda p: p['compound'])\n",
    "\n",
    "\n",
    "posts['rating'] = posts['compound'].apply(lambda score: 'pos' if score > 0 else 'neg' if score < 0 else 'neutral')\n",
    "#----------------------------------- FOR DATAFRAME ------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# clean_up(worldTitles)\n",
    "posts['rating'].value_counts()\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "topic_results = LDA.transform(dtm)\n",
    "posts['Topic By LDA'] = topic_results.argmax(axis=1)\n",
    "posts['Topic by NMF'] = topic_result.argmax(axis=1)\n",
    "posts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "npr = pd.read_csv(r'05-Topic-Modeling\\npr.csv')\n",
    "npr.head()\n",
    "\n",
    "# npr['Article'][3]\n",
    "# len(npr)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Removes words that appear in 90% of the documents, words that show up a minimum amout of times (min_df can either be a ratio or has to appear in atleast 2 documents), remove stop_words \n",
    "cv = CountVectorizer(max_df=0.9, min_df=2, stop_words='english')\n",
    "dtm = cv.fit_transform(npr['Article'])\n",
    "dtm\n",
    "# <11992x54777> Articles, Terms\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "# n_components, how many topics do I want. International, Local, National Politics\n",
    "LDA = LatentDirichletAllocation(n_components=7, random_state=42)\n",
    "LDA.fit(dtm)\n",
    "\n",
    "# Grab the Vocabulary of Words\n",
    "len(cv.get_feature_names_out())\n",
    "\n",
    "import random\n",
    "random_word_id = random.randint(0, len(cv.get_feature_names_out()))\n",
    "cv.get_feature_names_out()[random_word_id]\n",
    "\n",
    "# Grab the topics\n",
    "# LDA.components_\n",
    "# First topic\n",
    "single_topic = LDA.components_[0]\n",
    "\n",
    "# Take the single topics and figure out which index position we should be looking at for high probability words in the single topic.\n",
    "single_topic.argsort()\n",
    "\n",
    "import numpy as np\n",
    "arr = np.array([10, 200, 1])\n",
    "# Gives the index position that would sort this\n",
    "arr.argsort()\n",
    "\n",
    "# grab the last 10 values of argsort\n",
    "# single_topic.argsort()[-10:]\n",
    "top_ten_words = single_topic.argsort()[-10:]\n",
    "\n",
    "for index in top_ten_words:\n",
    "    print(cv.get_feature_names_out()[index])\n",
    "\n",
    "\n",
    "# Grab the highest probabilty words per topic\n",
    "\n",
    "# LDA components are just hte topics of the article\n",
    "# Of each topic, we are gettin g the top 15 words\n",
    "for index , topic in enumerate(LDA.components_):\n",
    "    print(f\"THE TOP 15 WORDS FOR TOPIC #{index}\")\n",
    "    print([cv.get_feature_names_out()[index] for index in topic.argsort()[-15:]])\n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "    \n",
    "    \n",
    "topic_results = LDA.transform(dtm)\n",
    "# Array (11992, 7) Articles, Topics\n",
    "# Probability that a documents belongs to a particular topic ^^^\n",
    "\n",
    "npr['Topic'] = topic_results.argmax(axis=1)\n",
    "npr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "npr = pd.read_csv(r'05-Topic-Modeling\\npr.csv')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_df=.9, min_df=2, stop_words='english')\n",
    "dtm = tfidf.fit_transform(npr['Article'])\n",
    "dtm \n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "nmf_model = NMF(n_components=7, random_state=42)\n",
    "nmf_model.fit(dtm)\n",
    "\n",
    "\n",
    "# LDA words with high probability\n",
    "# NMF words with high coeffecient values\n",
    "for index, topic in enumerate(nmf_model.components_):\n",
    "    print(f'The top 15 results for topic # {index}')\n",
    "    print([tfidf.get_feature_names_out()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')\n",
    "\n",
    "topic_results = nmf_model.transform(dtm)\n",
    "\n",
    "topic_results.argmax(axis=1)\n",
    "npr['Topic'] = topic_results.argmax(axis=1)\n",
    "\n",
    "mytopic_dict = {0: 'Health', 1: 'Politics', 2:'Legislation', 3:'Foreign Affairs', 4: 'Election', 5: 'Music', 6: 'Education'}\n",
    "npr['Topic Label'] = npr['Topic'].map(mytopic_dict)\n",
    "npr.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "# print(iris.DESCR)\n",
    "\n",
    "X = iris.data\n",
    "X\n",
    "# (SL, SW, PL, PW)\n",
    "# X\n",
    "\n",
    "y = iris.target\n",
    "# y \n",
    "\n",
    "# Want a vector that is zero for every value that the match does not match up\n",
    "# One Hot Encoding\n",
    "# class 0 --> [1, 0, 0]\n",
    "# class 1 --> [0, 1, 0]\n",
    "# class 2 --> [0, 0, 1]\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y = to_categorical(y)\n",
    "# y\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.33)\n",
    "\n",
    "# For Neural networks it is good to scale or standardize your data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# np.array([5, 10, 15, 20])/ 20\n",
    "scaler_object = MinMaxScaler()\n",
    "scaler_object.fit(X_train)\n",
    "\n",
    "\n",
    "scaled_X_train = scaler_object.transform(X_train)\n",
    "# print(scaled_X_train)\n",
    "scaled_X_test = scaler_object.transform(X_test)\n",
    "\n",
    "# scaled_X_train\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import metrics\n",
    "\n",
    "model = Sequential()\n",
    "# Dense(neurons, )\n",
    "model.add(Dense(8, input_dim=4, activation='relu'))\n",
    "model.add(Dense(8, input_dim=4, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "# 3 Neurons because each neuron will have a probability in each particular class [0, 0, 1] [1,0,0] [0,1,0]\n",
    "# Will be in probability / percentages\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# Verbose how much information you want bacj\n",
    "#model.fit(scaled_X_train, y_train, epochs=150, verbose=2)\n",
    "\n",
    "# Predict on new unseen data.\n",
    "# Have to scale if you add new data\n",
    "predict_x=model.predict(scaled_X_test)\n",
    "classes_x=np.argmax(predict_x,axis=1)\n",
    "classes_x\n",
    "\n",
    "# on y_test: the [00000000,11111,222222]\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "print(confusion_matrix(y_test.argmax(axis=1), classes_x))\n",
    "print(classification_report(y_test.argmax(axis=1), classes_x))\n",
    "\n",
    "model.save('myfirstmodel.h5')\n",
    "from keras.models import load_model\n",
    "new_model = load_model('myfirstmodel.h5')\n",
    "                                                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    with open(filepath) as f:   \n",
    "        str_text = f.read()\n",
    "    \n",
    "    return str_text\n",
    "\n",
    "# read_file(r'06-Deep-Learning\\moby_dick_four_chapters.txt')\n",
    "\n",
    "# read_file(r'06-Deep-Learning\\melville-moby_dick.txt')\n",
    "\n",
    "\n",
    "import spacy \n",
    "nlp = spacy.load(r'en_core_web_lg\\en_core_web_lg-3.2.0', disable=['parser', 'tagger', 'ner'])\n",
    "nlp.max_length = 1198623\n",
    "\n",
    "\n",
    "def seperate_punc(doc_text):\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']\n",
    "\n",
    "d = read_file(r'moby_dick_four_chapters.txt')\n",
    "\n",
    "tokens = seperate_punc(d)\n",
    "# len(tokens)\n",
    "\n",
    "# 25 words --> network predict #26\n",
    "train_len = 25 + 1\n",
    "text_sequence = []\n",
    "\n",
    "for i in range(train_len, len(tokens)):\n",
    "    seq = tokens[i-train_len:i]\n",
    "    text_sequence.append(seq)\n",
    "    \n",
    "# ' '.join(text_sequence[0]) First sentence ->>> call me ishmael\n",
    "# ' '.join(text_sequence[1]) Second sentence shifted by 1 ->>> me ishmael some \n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequence)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(text_sequence)\n",
    "# sequences[0]\n",
    "\n",
    "# Dictionary with index and word\n",
    "tokenizer.index_word\n",
    "\n",
    "# Number is not a count, just a unique ID\n",
    "# for i in sequences[0]:\n",
    "#    print(f\"{i} : {tokenizer.index_word[i]}\")\n",
    "\n",
    "\n",
    "# tokenizer.word_counts\n",
    "vocabulary_size = len(tokenizer.word_counts)\n",
    "\n",
    "import numpy as np\n",
    "sequences = np.array(sequences)\n",
    "sequences\n",
    "    \n",
    "# Preform feature train split- seperates first 25 as features, last one as predict\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# Grab every column except the last one\n",
    "X = sequences[:,:-1]\n",
    "# Rows: Columns\n",
    "# All the rows, last column\n",
    "y = sequences[:,-1]\n",
    "\n",
    "y = to_categorical(y, num_classes=vocabulary_size+1)\n",
    "seq_len = X.shape[1]\n",
    "# Sequences, how many words per sequence (11312, 25) X.shape\n",
    "\n",
    "from keras.models import Sequential\n",
    "# Dense for layers, LSTM to deal with sequences, embedding for vocabulary\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "\n",
    "\n",
    "# Input, Output \n",
    "def create_model(vocabulary_size, seq_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, seq_len, input_length=seq_len))\n",
    "    model.add(LSTM(seq_len*2, return_sequences=True))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(vocabulary_size+1, seq_len)\n",
    "\n",
    "# Save the file and load it later\n",
    "from pickle import dump, load\n",
    "model.fit(X, y, batch_size=128, epochs=2, verbose=1)\n",
    "\n",
    "model.save('my_mobydick_model.h5')\n",
    "dump(tokenizer, open('my_simpletokenizer', 'wb'))\n",
    "\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "    output_text = []\n",
    "    \n",
    "    input_text = seed_text\n",
    "    \n",
    "    for i in range(num_gen_words):\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        \n",
    "        # If you have too many words, makes sure its only 25.\n",
    "        # If too little, pads it up to 25\n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
    "\n",
    "        \n",
    "        # Predict class probability for each word. Assign a probability for the most likely next word\n",
    "        pred_word_inds = model.predict(pad_encoded, verbose=0)[0]\n",
    "        pred_word_ind = np.argmax(pred_word_inds, axis=0)\n",
    "        \n",
    "        pred_word = tokenizer.index_word[pred_word_ind]\n",
    "        input_text += ' '+pred_word\n",
    "        \n",
    "        output_text.append(pred_word)\n",
    "    \n",
    "    return ' '.join(output_text)\n",
    "\n",
    "# generate_text(model, tokenizer, seq_len, seed_text, num_gen_words)\n",
    "\n",
    "\n",
    "import random\n",
    "random.seed(101)\n",
    "random_pick = random.randint(0, len(text_sequence))\n",
    "\n",
    "random_seed_text = text_sequence[random_pick]\n",
    "random_seed_text\n",
    "\n",
    "seed_text = ' '.join(random_seed_text)\n",
    "seed_text\n",
    "\n",
    "generate_text(model, tokenizer, seq_len, seed_text=seed_text, num_gen_words=25)\n",
    "\n",
    "import pickle\n",
    "\n",
    "filename = r'my_mobydick.h5'\n",
    "\n",
    "with open(r'my_mobydick_model.h5') as f:\n",
    "    \n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# 60,000 images, each 28x28 pixels\n",
    "train_images.shape\n",
    "\n",
    "# Feed the Neural network the training data -> train_images, train_labels\n",
    "# It will learn to associate images and labels\n",
    "# Ask to produce predictions for the testing data\n",
    "\n",
    "from keras import models, layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "# Layer - data-processing module that acts as a filter for data.\n",
    "# Deep learning consists of chaining the smaller layers that will implement a form of progressive data distiliation --> process of transferring knowledge from a large model to a smaller one\n",
    "\n",
    "# Make the model ready for training \n",
    "#    -Loss function: How the network will measure its performance on training data (Steer in right direction)\n",
    "#    -Optimizer: Mechanism where the network updates itself based on the data it sees and loss funct\n",
    "#    -Metrics to monitor during training and testing: Accuracy, confusion matrix, classification report\n",
    "\n",
    "\n",
    "# The Compilation step\n",
    "network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Previously, was stored in shape (60000, 28, 28). Tranform into shape (60000, 28 * 28) with values between 0 and 1\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "\n",
    "# Categorically encode the labels\n",
    "# One Hot Encoding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Ready to fit the model to its training data\n",
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "# Two Quantities are displayed: The loss of the network over the training and the accuracy of the network\n",
    "# Model reaches an accuracy of .989 (98.9%)\n",
    "# Check what the model performs on the test set\n",
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "\n",
    "# print('test_loss:', test_loss)\n",
    "# print('test_acc:', test_acc)\n",
    "# .07% loss. Model is slightly overfitted: performs worse on test data than train\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Tensor = Arrays\n",
    "\n",
    "# tells you the amount of axis on the tensor. In this case: 2\n",
    "# print(train_images.ndim)\n",
    "\n",
    "# Our current model is a 3D tensor of 8-bit integers. Array of 60000 matrices of 28*28 integers. Each matrix is a grawscale image, with values 0-255\n",
    "# print(train_images.shape) --> (60000, 28, 28)\n",
    "# print(train_images.dtype) --> uint8\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# You can split pixel image\n",
    "my_slice = train_images[10:100]\n",
    "# Shape (90, 28, 28)\n",
    "# Select 14 x 14 pixels in the bottom-right corner of all images:\n",
    "my_slice = train_images[:, 14, 14]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Text Generation:\n",
    "-LSTM(Long Short-Term Memory)\n",
    "Random Sampling to produce more realistic and unrepetitive words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "with open(r'train_qa.txt', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "with open(r'test_qa.txt', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "len(train_data)\n",
    "' '.join(train_data[0][0])\n",
    "train_data[0][2]\n",
    "\n",
    "all_data = test_data + train_data\n",
    "len(all_data)\n",
    "\n",
    "\n",
    "vocab = set()\n",
    "\n",
    "for story, question, answer in all_data:\n",
    "    vocab = vocab.union(set(story))\n",
    "    vocab = vocab.union(set(question))\n",
    "    \n",
    "vocab.add('no')\n",
    "vocab.add('yes')\n",
    "\n",
    "vocab_len = len(vocab) + 1\n",
    "\n",
    "all_story_lens = [len(data[0]) for data in all_data]\n",
    "max_story_len = max(all_story_lens)\n",
    "\n",
    "max_question_len = max([len(data[1]) for data in all_data])\n",
    "max_question_len\n",
    "\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(filters=[])\n",
    "tokenizer.fit_on_texts(vocab)\n",
    "\n",
    "tokenizer.word_index\n",
    "\n",
    "train_story_text = []\n",
    "train_question_text = []\n",
    "train_answers = []\n",
    "\n",
    "for story, question, answers in train_data:\n",
    "    train_story_text.append(story)\n",
    "    train_question_text.append(question)\n",
    "    train_answers.append(answers)\n",
    "\n",
    "\n",
    "# List with three \n",
    "# train_story_text \n",
    "\n",
    "train_story_seq = tokenizer.texts_to_sequences(train_story_text)\n",
    "train_story_seq\n",
    "\n",
    "def vectorize_stories(data, word_index=tokenizer.word_index, max_story_len=max_story_len, max_question_len=max_question_len):\n",
    "    # Stories = X\n",
    "    X = []\n",
    "    # Questions = Xq\n",
    "    Xq = []\n",
    "    # Y = Target = Correct Answer (yes/no)\n",
    "    Y = []\n",
    "    \n",
    "    for story, question, answer in data:\n",
    "        # for each story\n",
    "        # Example: [23, 14, 15....]\n",
    "        x = [word_index[word.lower()] for word in story]\n",
    "        xq = [word_index[word.lower()] for word in question]\n",
    "        \n",
    "        # Since we're using pad sequence, index zero is taken\n",
    "        y = np.zeros(len(word_index) + 1)\n",
    "        y[word_index[answer]] = 1\n",
    "        \n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "    \n",
    "    return (pad_sequences(X, maxlen=max_story_len), pad_sequences(Xq, maxlen=max_question_len), np.array(Y))\n",
    "\n",
    "inputs_train, queries_train, answers_train = vectorize_stories(train_data)\n",
    "inputs_test, queries_test, answers_test = vectorize_stories(test_data)\n",
    "\n",
    "inputs_test        \n",
    "\n",
    "# 34 \n",
    "tokenizer.word_index['yes']\n",
    "# 4\n",
    "tokenizer.word_index['no']   \n",
    "sum(answers_test)  \n",
    " \n",
    "# 503: No, 496: Yes\n",
    "\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout, add, dot, concatenate, LSTM\n",
    "\n",
    "# PLACEHOLDER shape=(max_story_len, batch_size)\n",
    "input_sequence = Input((max_story_len,))\n",
    "question = Input((max_question_len,))\n",
    "\n",
    "# vocab_len\n",
    "vocab_size = len(vocab) + 1\n",
    "\n",
    "# INPUT ENCODER M\n",
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim=vocab_size, output_dim=64))\n",
    "input_encoder_m.add(Dropout(0.3))\n",
    "\n",
    "# (samples, stories_maxlen, embedding_dim)\n",
    "\n",
    "# Input ENCODER C\n",
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim=vocab_size, output_dim=max_question_len))\n",
    "input_encoder_c.add(Dropout(0.3))\n",
    "\n",
    "#OUTPUT\n",
    "#(samples,story_maxlen_max_question_len)\n",
    "\n",
    "\n",
    "# QUESTION ENCODER\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_size, output_dim=64,input_length=max_question_len))\n",
    "question_encoder.add(Dropout(0.3))\n",
    "\n",
    "# (samples,question_maxlen, embedding_dim)\n",
    "\n",
    "\n",
    "# Result of Encoder -> Encoded\n",
    "input_encoded_m = input_encoder_m(input_sequence)\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "question_encoded = question_encoder(question)\n",
    "\n",
    "match = dot([input_encoded_m, question_encoded], axes=(2,2))\n",
    "match = Activation('softmax')(match)\n",
    "\n",
    "\n",
    "# Converting to have output of samples by question max_len to story max_len\n",
    "response = add([match, input_encoded_c])\n",
    "response = Permute((2,1))(response)\n",
    "\n",
    "answer = concatenate([response, question_encoded])\n",
    "answer\n",
    "\n",
    "\n",
    "answer = LSTM(32)(answer)\n",
    "answer = Dropout(0.5)(answer)\n",
    "answer = Dense(vocab_size)(answer) # (samples,vocab_size) # YES/NO 0000\n",
    "\n",
    "# Probability matrix\n",
    "answer = Activation('softmax')(answer)\n",
    "\n",
    "model = Model([input_sequence, question], answer)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Import the Libaries\n",
    "*   First step: Import the required libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras module for building LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.utils as ku\n",
    "\n",
    "# Set seeds for repdocability\n",
    "from tensorflow.random import set_seed\n",
    "from numpy.random import seed\n",
    "\n",
    "# What does this do ------\n",
    "set_seed(2)\n",
    "seed(1)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os\n",
    "\n",
    "\n",
    "curr_dir = r'\\Desktop\\Data'\n",
    "all_headlines = []\n",
    "\n",
    "for filename in os.listdir(curr_dir):\n",
    "    if 'Articles' in filename:\n",
    "        curr_file = curr_dir + '\\\\' + filename\n",
    "        print(filename, curr_file)\n",
    "        article_df = pd.read_csv(curr_file)\n",
    "        all_headlines.extend(list(article_df.headline.values))\n",
    "        break\n",
    "\n",
    "all_headlines = [h for h in all_headlines if h != 'Unknown']\n",
    "# len = 831\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data Preparation\n",
    "###   Dataset Cleaning\n",
    "\n",
    "*   Perform text cleaning of the data which includes the removal of punctuations and lower casing \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt): \n",
    "    txt = ''.join(v for v in txt if v not in string.punctuation).lower()\n",
    "    txt = txt.encode('utf8').decode('ascii', 'ignore')\n",
    "    \n",
    "    return txt\n",
    "\n",
    "corpus = [clean_text(x) for x in all_headlines]\n",
    "print(all_headlines[:10], '\\n\\n')\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Sequences of N-gram Tokens\n",
    "*   Language modeling requires a sequence of input data, as given a sequence of words/tokens. The aim is to predict the next word/token\n",
    "*   Next Step is tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    # Convert data to sequence of tokens\n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    \n",
    "    return input_sequences, total_words\n",
    "\n",
    "inp_sequences, total_words = get_sequence_of_tokens(corpus)\n",
    "inp_sequences[:10]\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Every number represents the ngram phrases generated from the input data.\n",
    "*   Every integer corresponds to the index of a partical word in the complete vocabulary of words present in the text\n",
    "\n",
    "### Padding the Sequences to obtain variables\n",
    "*   Since every headline may have a different length, have to pad so that the lenghts are equal.\n",
    "\n",
    "*   To input the data into a learning model, you have to create predictors and labels\n",
    "\n",
    "### Headline: they are learning data science\n",
    "| Predictors            | Label           |\n",
    "|-----------------------|-----------------|\n",
    "| they                  | are             |\n",
    "| they are              | learning        |\n",
    "| they are learning     | data            |\n",
    "| they are learning data| science         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "    \n",
    "    predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "    label = to_categorical(label, num_classes= total_words)\n",
    "    \n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMs for Text Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    \n",
    "    # Add Hidden Layer 1 - LSTM layer\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(.1))\n",
    "    \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.fit(predictors, label, epochs=5, verbose=5)\n",
    "\n",
    "\n",
    "\n",
    "model.save('temporary.h5')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the Text\n",
    "* Can train the model after it finishes fitting\n",
    "* Function predicts the next word based on the input words (seed text)\n",
    "* Tokenize the seed text -> Pad the sequences -> pass into the trained model to get predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        \n",
    "        predict = model.predict(token_list)\n",
    "        # predicted = model.predict_classes(token_list, verbose=0)\n",
    "        classes = np.argmax(predict, axis=1)\n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == classes:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()   \n",
    "\n",
    "\n",
    "\n",
    "generate_text('Covid', 8, new_model, max_sequence_len)\n",
    "# generate_text('Obama', 8, model, max_sequence_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "089e656c5fa2aeeafc83cac1ecf04baff896192e19b6c00362e69b8911004b02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
